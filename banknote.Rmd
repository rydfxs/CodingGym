---
title: "Bank Authentication - R Supervised Learning"
output:
  html_document: default
  html_notebook: default
---

Project: Bank Authentication

Player: Jin Wang

This is an R warmup exercise. The aim of this is try to find an automated and trustable way to check the banknote.


Data were extracted from images that were taken from genuine and forged banknote-like specimens. For digitization, an industrial camera usually used for print inspection was used. The final images have 400x 400 pixels. Due to the object lens and distance to the investigated object gray-scale pictures with a resolution of about 660 dpi were gained. Wavelet Transform tool were used to extract features from images.

Details of the Wavelet transform can be found:
https://en.wikipedia.org/wiki/Wavelet_transform


Attribute Information:

1. variance of Wavelet Transformed image (continuous) 
2. skewness of Wavelet Transformed image (continuous) 
3. curtosis of Wavelet Transformed image (continuous) 
4. entropy of image (continuous) 
5. class (integer) 


![](./catchme.jpg)

```{r}
# Import the libraries

library(psych)
library(dplyr)
library(ggplot2)
library(readr)
library(caret)
```




```{r}

# read and check the data structures

df <- read_csv("data_banknote_authentication.csv")

# Check the structure
glimpse(df)

# Check mean, sd, summar info
describe(df)

```

We have the image variants, skewness,kurtosis and entropy and then we have class indicate if banknote is authentic.

Now check the top 6 rows.
```{r}
library(DT)
datatable(head(df))
```


Explore the correlation and distribution
```{r}
pairs.panels(df)
```

Generally, if it is quantiative value with less nosie, standardization is needed.
I didn't scale or normalized the data, because they are at the same order of magnitude and they have small ranges.
It may not be a big deal if not normalize the data. I still can do it when training the data.

```{r}
# Explore the data between Explained variable and Explanatory variable

library(plotly)
ggplotly(ggplot(data = df, aes(x = Image.Var, y = Image.Skew, col = class)) + geom_point()+ theme_bw())
ggplotly(ggplot(data = df, aes(x = Image.Curt, y = Entropy, col = class)) + geom_point() + theme_bw())
ggplotly(ggplot(data = df, aes(x = Entropy, y = Image.Skew, col = class)) + geom_jitter() + theme_bw())

```

Convert the explanatory variable to factor since it is two class classification issue
```{r}
df$class <- as.factor(df$class)
```



Split the data to 70/30
```{r}
inTrain <- createDataPartition( y = df$class, p = .7, list = FALSE)
train <- df[inTrain,]
test <- df[-inTrain,]
```



Check if contain NULL data.
If contain NULL data, prepare to impute them with KNN
```{r}
library(Amelia)
missmap(train, main = "Missing Value")
```
No Null data, good





Now create Train control - 10 fold cross validation.
```{r}
fitControl <- trainControl(method = "repeatedcv", repeats = 2, number = 10  )
```


Train the model and preProcess scale the data.
```{r}
set.seed(1234)

## Try the random forest model

model.rf <- train(data = train, class ~., method = "ranger", trControl = fitControl , preProcess = c("center", "scale")) 

## Try the generalized linear model with penalized maximum likelihood

model.net <- train(data = train, class ~., method = "glmnet", trControl = fitControl ,  preProcess = c("center", "scale"), tuneGrid = expand.grid(alpha = 0:1, lambda = seq(0.0001,1,length = 20)) )
```

```{r}
print(model.rf)
```

Check the generalized linear model
```{r}
print(model.net)
```

Predict on the test dataset by using random forest
```{r}
test.rf <- predict(model.rf, test)
```


Using confusion Matrix to check the accuracy and recall of random forest model
```{r}
confusionMatrix(data = test.rf, reference = test$class)

```


Using confusion Matrix to check the accuracy and recall of generalized linear model 
```{r}
test.net <- predict(model.net, test)
confusionMatrix(test.net, reference = test$class)
```

Compare two model choose the high accuracy linear regular model.
The accuracy is 99.5%. 
